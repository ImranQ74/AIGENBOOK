# AIGENBOOK Environment Configuration
# Copy this file to .env and fill in your values

# ===========================================
# Server Configuration
# ===========================================
HOST=0.0.0.0
PORT=8000
DEBUG=false

# ===========================================
# Qdrant Vector Database
# ===========================================
# For local development (in-memory), leave these empty
# For cloud Qdrant, configure:
QDRANT_URL=
QDRANT_API_KEY=
QDRANT_COLLECTION_NAME=aigenbook_chunks

# ===========================================
# Neon PostgreSQL (Optional)
# ===========================================
# Get free tier at https://neon.tech
NEON_DATABASE_URL=

# ===========================================
# Embedding Model Configuration
# ===========================================
EMBEDDING_MODEL=all-MiniLM-L6-v2
EMBEDDING_DEVICE=cpu  # Use "cuda" for GPU

# ===========================================
# LLM Configuration
# ===========================================
# Recommended: Use OpenAI-compatible API for fast responses
# Free-tier options: Groq (groq.com), Together AI, etc.
LLM_PROVIDER=openai
LLM_MODEL=gpt-3.5-turbo

# For OpenAI-compatible API:
# Groq example: https://api.groq.com/openai/v1
LLM_API_BASE=https://api.groq.com/openai/v1
LLM_API_KEY=gsk_...

# ===========================================
# RAG Configuration
# ===========================================
CHUNK_SIZE=500
CHUNK_OVERLAP=50
TOP_K=5
SIMILARITY_THRESHOLD=0.5
