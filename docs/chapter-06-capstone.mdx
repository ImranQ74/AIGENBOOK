---
title: Chapter 6 - Capstone: Simple AI-Robot Pipeline
description: Build a complete AI-robot pipeline from perception to action
---

# Capstone: Simple AI-Robot Pipeline

This chapter guides you through building a complete robot perception-to-action pipeline integrating all concepts from previous chapters.

## 6.1 Project Overview

### Goal
Build a robot system that:
1. Perceives objects in its environment
2. Understands natural language commands
3. Plans and executes manipulation tasks

### Architecture

```
┌─────────────────────────────────────────────────────────────────────┐
│                    CAPSTONE PIPELINE                                │
├─────────────────────────────────────────────────────────────────────┤
│                                                                     │
│   ┌──────────┐     ┌──────────┐     ┌──────────┐     ┌──────────┐ │
│   │  RGB-D   │────>│  Object  │────>│   VLA    │────>│  Robot   │ │
│   │  Camera  │     │ Detection│     │  Model   │     │  Control │ │
│   └──────────┘     └──────────┘     └──────────┘     └──────────┘ │
│        │                                    │                       │
│        │              ┌─────────────────────┘                       │
│        │              │                                               │
│        ▼              ▼                                               │
│   ┌──────────┐     ┌──────────┐                                      │
│   │  Depth   │     │  Task    │                                      │
│   │  Map     │     │  Planner │                                      │
│   └──────────┘     └──────────┘                                      │
│                                                                     │
└─────────────────────────────────────────────────────────────────────┘
```

## 6.2 Project Structure

```
ai_robot_pipeline/
├── config/
│   ├── robot_config.yaml
│   └── camera_config.yaml
├── src/
│   ├── perception/
│   │   ├── __init__.py
│   │   └── object_detector.py
│   ├── planning/
│   │   ├── __init__.py
│   │   └── task_planner.py
│   ├── control/
│   │   ├── __init__.py
│   │   └── robot_controller.py
│   └── main.py
├── scripts/
│   └── run_pipeline.py
├── tests/
│   └── test_pipeline.py
├── package.xml
└── setup.py
```

## 6.3 Step 1: Perception Module

### Object Detection with YOLO

```python
# src/perception/object_detector.py
import cv2
import numpy as np
from typing import List, Dict, Tuple
from ultralytics import YOLO

class ObjectDetector:
    """
    Detects and localizes objects in the scene using YOLO.
    """

    def __init__(self, model_path: str = "yolov8n.pt"):
        self.model = YOLO(model_path)
        self.class_names = self.model.names

    def detect(
        self,
        rgb_image: np.ndarray,
        conf_threshold: float = 0.5
    ) -> List[Dict]:
        """
        Detect objects in RGB image.

        Args:
            rgb_image: H x W x 3 BGR image
            conf_threshold: Minimum confidence score

        Returns:
            List of detections with bounding boxes and classes
        """
        results = self.model(rgb_image, conf=conf_threshold, verbose=False)

        detections = []
        for result in results[0].boxes:
            detection = {
                'class_id': int(result.cls),
                'class_name': self.class_names[int(result.cls)],
                'confidence': float(result.conf),
                'bbox': result.xyxy.cpu().numpy().flatten().tolist(),
            }
            detections.append(detection)

        return detections

    def detect_with_depth(
        self,
        rgb_image: np.ndarray,
        depth_image: np.ndarray,
        camera_intrinsics: Dict
    ) -> List[Dict]:
        """
        Detect objects and estimate 3D positions.

        Args:
            rgb_image: H x W x 3 BGR image
            depth_image: H x W depth map in meters
            camera_intrinsics: Camera calibration parameters

        Returns:
            List of detections with 3D positions
        """
        detections = self.detect(rgb_image)

        fx, fy, cx, cy = (
            camera_intrinsics['fx'],
            camera_intrinsics['fy'],
            camera_intrinsics['cx'],
            camera_intrinsics['cy'],
        )

        for det in detections:
            bbox = det['bbox']
            x1, y1, x2, y2 = map(int, bbox)

            # Center of bounding box
            u = (x1 + x2) // 2
            v = (y1 + y2) // 2

            # Get depth at center
            depth = depth_image[v, u] if 0 <= v < depth_image.shape[0] and 0 <= u < depth_image.shape[1] else 0

            # Back-project to 3D
            det['position_3d'] = {
                'x': (u - cx) * depth / fx,
                'y': (v - cy) * depth / fy,
                'z': depth,
            }

        return detections
```

### Camera Integration

```python
# src/perception/camera.py
import cv2
import numpy as np
from typing import Tuple, Optional

class RGBDCamera:
    """
    Interface for RGB-D camera (RealSense, Azure Kinect, etc.)
    """

    def __init__(self, camera_id: int = 0):
        self.cap = cv2.VideoCapture(camera_id)
        self.cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)
        self.cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)
        self.cap.set(cv2.CAP_PROP_FPS, 30)

        # Default intrinsics (should be calibrated)
        self.intrinsics = {
            'fx': 500.0,
            'fy': 500.0,
            'cx': 320.0,
            'cy': 240.0,
        }

    def read(self) -> Tuple[Optional[np.ndarray], Optional[np.ndarray]]:
        """
        Read synchronized RGB and depth frames.

        Returns:
            Tuple of (rgb_image, depth_image) or (None, None) if failed
        """
        ret, rgb = self.cap.read()
        if not ret:
            return None, None

        # For demo, create synthetic depth
        # In practice, read from aligned depth stream
        depth = self._create_synthetic_depth(rgb)

        return rgb, depth

    def _create_synthetic_depth(self, rgb: np.ndarray) -> np.ndarray:
        """Create synthetic depth for demo purposes."""
        h, w = rgb.shape[:2]
        depth = np.ones((h, w), dtype=np.float32) * 2.0  # 2 meters default

        # Add some variation
        y, x = np.ogrid[:h, :w]
        center_mask = ((x - w/2)**2 + (y - h/2)**2) < (w/4)**2
        depth[center_mask] = 1.5

        return depth

    def release(self):
        """Release camera resources."""
        self.cap.release()

    def __enter__(self):
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        self.release()
```

## 6.4 Step 2: Task Planning

```python
# src/planning/task_planner.py
from typing import List, Dict, Optional
from enum import Enum

class TaskStatus(Enum):
    PENDING = "pending"
    IN_PROGRESS = "in_progress"
    COMPLETED = "completed"
    FAILED = "failed"

class TaskPlanner:
    """
    Simple task planner that converts high-level commands
    to robot actions.
    """

    # Mapping of commands to actions
    COMMAND_MAP = {
        'pick up': {'action': 'pick', 'gripper': 'close'},
        'grab': {'action': 'pick', 'gripper': 'close'},
        'pick': {'action': 'pick', 'gripper': 'close'},
        'put down': {'action': 'place', 'gripper': 'open'},
        'place': {'action': 'place', 'gripper': 'open'},
        'drop': {'action': 'place', 'gripper': 'open'},
        'move to': {'action': 'move', 'gripper': 'open'},
        'go to': {'action': 'move', 'gripper': 'open'},
    }

    def __init__(self, workspace_bounds: Dict = None):
        self.workspace_bounds = workspace_bounds or {
            'x_min': -0.5, 'x_max': 0.5,
            'y_min': -0.5, 'y_max': 0.5,
            'z_min': 0.0, 'z_max': 0.5,
        }

    def parse_command(
        self,
        command: str,
        detected_objects: List[Dict]
    ) -> Optional[Dict]:
        """
        Parse natural language command into actionable task.

        Args:
            command: Natural language instruction
            detected_objects: List of detected objects

        Returns:
            Parsed task or None if parsing fails
        """
        command = command.lower()

        # Find action
        action = None
        for cmd, action_map in self.COMMAND_MAP.items():
            if cmd in command:
                action = action_map['action']
                break

        if action is None:
            return None

        # Find target object
        target_object = self._find_object(command, detected_objects)
        if target_object is None and action in ['pick', 'place']:
            return None

        task = {
            'action': action,
            'target': target_object,
            'status': TaskStatus.PENDING,
        }

        return task

    def _find_object(
        self,
        command: str,
        objects: List[Dict]
    ) -> Optional[Dict]:
        """Find the object mentioned in the command."""
        for obj in objects:
            if obj['class_name'] in command:
                return obj
        return None

    def plan_motion(
        self,
        task: Dict,
        current_pose: Dict = None
    ) -> List[Dict]:
        """
        Generate motion plan for completing the task.

        Args:
            task: Parsed task dictionary
            current_pose: Current end-effector pose

        Returns:
            List of motion waypoints
        """
        if task['action'] == 'pick':
            return self._plan_pick(task['target'], current_pose)
        elif task['action'] == 'place':
            return self._plan_place(task['target'], current_pose)
        elif task['action'] == 'move':
            return self._plan_move(task['target'], current_pose)
        return []

    def _plan_pick(
        self,
        target: Dict,
        current_pose: Dict = None
    ) -> List[Dict]:
        """Plan pick motion."""
        obj_pos = target['position_3d']

        pre_grasp = {
            'position': {
                'x': obj_pos['x'],
                'y': obj_pos['y'],
                'z': obj_pos['z'] + 0.1,
            },
            'gripper': 'open',
        }

        grasp = {
            'position': {
                'x': obj_pos['x'],
                'y': obj_pos['y'],
                'z': obj_pos['z'] + 0.02,
            },
            'gripper': 'close',
        }

        lift = {
            'position': {
                'x': obj_pos['x'],
                'y': obj_pos['y'],
                'z': obj_pos['z'] + 0.1,
            },
            'gripper': 'close',
        }

        return [pre_grasp, grasp, lift]

    def _plan_place(
        self,
        target: Dict,
        current_pose: Dict = None
    ) -> List[Dict]:
        """Plan place motion."""
        place_pos = target.get('position_3d', {'x': 0.3, 'y': 0.0, 'z': 0.05})

        approach = {
            'position': {
                'x': place_pos['x'],
                'y': place_pos['y'],
                'z': place_pos['z'] + 0.1,
            },
            'gripper': 'close',
        }

        place = {
            'position': {
                'x': place_pos['x'],
                'y': place_pos['y'],
                'z': place_pos['z'] + 0.02,
            },
            'gripper': 'open',
        }

        retreat = {
            'position': {
                'x': place_pos['x'],
                'y': place_pos['y'],
                'z': place_pos['z'] + 0.1,
            },
            'gripper': 'open',
        }

        return [approach, place, retreat]

    def _plan_move(
        self,
        target: Dict,
        current_pose: Dict = None
    ) -> List[Dict]:
        """Plan move motion."""
        target_pos = target.get('position_3d', {'x': 0.3, 'y': 0.0, 'z': 0.2})

        return [{
            'position': {
                'x': target_pos['x'],
                'y': target_pos['y'],
                'z': target_pos['z'],
            },
            'gripper': 'open',
        }]
```

## 6.5 Step 3: Robot Control

```python
# src/control/robot_controller.py
import time
from typing import Dict, List, Optional
import numpy as np

class RobotController:
    """
    Interface for robot control.
    Supports both real robots and simulation.
    """

    def __init__(self, is_simulation: bool = True):
        self.is_simulation = is_simulation
        self.current_pose = {
            'position': {'x': 0.3, 'y': 0.0, 'z': 0.3},
            'gripper': 'open',
        }
        self.gripper_state = 'open'

    def move_to_pose(
        self,
        target_pose: Dict,
        speed: float = 0.1,
        timeout: float = 10.0
    ) -> bool:
        """
        Move end-effector to target pose.

        Args:
            target_pose: Target position and gripper state
            speed: Movement speed in m/s
            timeout: Maximum movement time in seconds

        Returns:
            True if successful, False otherwise
        """
        start_time = time.time()

        # Calculate trajectory
        start_pos = np.array([
            self.current_pose['position']['x'],
            self.current_pose['position']['y'],
            self.current_pose['position']['z'],
        ])

        target_pos = np.array([
            target_pose['position']['x'],
            target_pose['position']['y'],
            target_pose['position']['z'],
        ])

        distance = np.linalg.norm(target_pos - start_pos)
        steps = int(distance / (speed * 0.016))  # 60Hz control

        # Execute trajectory
        for i in range(steps):
            if time.time() - start_time > timeout:
                return False

            alpha = (i + 1) / steps
            interp_pos = start_pos + alpha * (target_pos - start_pos)

            self.current_pose['position'] = {
                'x': float(interp_pos[0]),
                'y': float(interp_pos[1]),
                'z': float(interp_pos[2]),
            }

            # In simulation, just update pose
            if self.is_simulation:
                time.sleep(0.016)
            else:
                self._send_joint_command(interp_pos)

        # Update gripper state
        if 'gripper' in target_pose:
            self._set_gripper(target_pose['gripper'])

        return True

    def _set_gripper(self, state: str):
        """Control gripper open/close state."""
        self.gripper_state = state
        # Actual hardware command would go here
        print(f"Gripper: {state}")

    def _send_joint_command(self, position: np.ndarray):
        """Send joint command to robot hardware."""
        # For real robot: send to motor controllers
        pass

    def get_current_pose(self) -> Dict:
        """Get current end-effector pose."""
        return self.current_pose.copy()

    def stop(self):
        """Emergency stop."""
        print("Emergency stop activated")
        self.current_pose = {
            'position': {'x': 0.0, 'y': 0.0, 'z': 0.2},
            'gripper': 'open',
        }
```

## 6.6 Step 4: Main Pipeline

```python
# src/main.py
#!/usr/bin/env python3
"""
Main entry point for the AI-Robot Pipeline.
"""

import cv2
import argparse
from typing import Optional

from perception.object_detector import ObjectDetector
from perception.camera import RGBDCamera
from planning.task_planner import TaskPlanner
from control.robot_controller import RobotController


class AIRobotPipeline:
    """
    Complete AI-Robot Pipeline integrating perception, planning, and control.
    """

    def __init__(
        self,
        use_simulation: bool = True,
        camera_id: int = 0
    ):
        self.detector = ObjectDetector()
        self.camera = RGBDCamera(camera_id)
        self.planner = TaskPlanner()
        self.controller = RobotController(is_simulation=use_simulation)

    def run(
        self,
        command: str,
        display: bool = True
    ) -> bool:
        """
        Execute a natural language command.

        Args:
            command: Natural language instruction
            display: Whether to show camera feed

        Returns:
            True if command completed successfully
        """
        # Step 1: Capture perception data
        rgb, depth = self.camera.read()
        if rgb is None:
            print("Failed to capture image")
            return False

        # Step 2: Detect objects
        objects = self.detector.detect_with_depth(
            rgb, depth, self.camera.intrinsics
        )

        print(f"Detected {len(objects)} objects:")
        for obj in objects:
            print(f"  - {obj['class_name']}: {obj['confidence']:.2f}")

        # Step 3: Parse command
        task = self.planner.parse_command(command, objects)
        if task is None:
            print(f"Could not parse command: '{command}'")
            return False

        print(f"Task: {task['action'].upper()} {task.get('target', {}).get('class_name', 'unknown')}")

        # Step 4: Plan motion
        waypoints = self.planner.plan_motion(
            task, self.controller.get_current_pose()
        )

        # Step 5: Execute
        success = True
        for i, waypoint in enumerate(waypoints):
            print(f"Executing waypoint {i+1}/{len(waypoints)}")
            result = self.controller.move_to_pose(waypoint)
            if not result:
                print(f"Failed at waypoint {i+1}")
                success = False
                break

        # Step 6: Display (for debugging)
        if display:
            self._visualize(rgb, objects, command)

        return success

    def _visualize(
        self,
        rgb: np.ndarray,
        objects: list,
        command: str
    ):
        """Visualize detection results."""
        for obj in objects:
            bbox = obj['bbox']
            x1, y1, x2, y2 = map(int, bbox)

            # Draw bounding box
            cv2.rectangle(rgb, (x1, y1), (x2, y2), (0, 255, 0), 2)

            # Draw label
            label = f"{obj['class_name']}: {obj['confidence']:.2f}"
            cv2.putText(rgb, label, (x1, y1 - 10),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)

        # Add command text
        cv2.putText(rgb, f"Command: {command}", (10, 30),
                   cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 0, 0), 2)

        cv2.imshow("AI Robot Pipeline", rgb)
        cv2.waitKey(1000)
        cv2.destroyAllWindows()

    def cleanup(self):
        """Release resources."""
        self.camera.release()


def main():
    parser = argparse.ArgumentParser(description="AI-Robot Pipeline Demo")
    parser.add_argument(
        "--camera", type=int, default=0, help="Camera device ID"
    )
    parser.add_argument(
        "--sim", action="store_true", default=True,
        help="Run in simulation mode"
    )
    parser.add_argument(
        "--command", type=str, default="pick up the cup",
        help="Natural language command"
    )
    args = parser.parse_args()

    # Create and run pipeline
    pipeline = AIRobotPipeline(
        use_simulation=args.sim,
        camera_id=args.camera
    )

    try:
        success = pipeline.run(args.command)
        print(f"Command {'completed successfully' if success else 'FAILED'}")
    finally:
        pipeline.cleanup()


if __name__ == "__main__":
    main()
```

## 6.7 ROS 2 Integration

```python
# src/ros2_wrapper.py
"""
ROS 2 wrapper for the AI-Robot Pipeline.
"""
import rclpy
from rclpy.node import Node
from std_msgs.msg import String
from sensor_msgs.msg import Image, CameraInfo
from geometry_msgs.msg import PoseStamped
from cv_bridge import CvBridge

class AIRobotNode(Node):
    """
    ROS 2 node wrapping the AI-Robot Pipeline.
    """

    def __init__(self):
        super().__init__('ai_robot_node')

        # Initialize pipeline
        self.pipeline = AIRobotPipeline(use_simulation=False)

        # ROS interfaces
        self.bridge = CvBridge()
        self.latest_image = None

        # Publishers
        self.status_pub = self.create_publisher(String, '/ai_robot/status', 10)
        self.pose_pub = self.create_publisher(PoseStamped, '/ai_robot/target_pose', 10)

        # Subscribers
        self.cmd_sub = self.create_subscription(
            String, '/ai_robot/command', self.on_command, 10
        )
        self.image_sub = self.create_subscription(
            Image, '/camera/color/image_raw', self.on_image, 10
        )

        self.get_logger().info("AI Robot Node initialized")

    def on_command(self, msg: String):
        """Handle incoming command."""
        self.get_logger().info(f"Received command: {msg.data}")

        success = self.pipeline.run(msg.data)

        status_msg = String()
        status_msg.data = 'success' if success else 'failed'
        self.status_pub.publish(status_msg)

    def on_image(self, msg: Image):
        """Handle incoming image."""
        self.latest_image = self.bridge.imgmsg_to_cv2(msg, 'bgr8')

    def cleanup(self):
        self.pipeline.cleanup()


def main(args=None):
    rclpy.init(args=args)
    node = AIRobotNode()

    try:
        rclpy.spin(node)
    except KeyboardInterrupt:
        pass
    finally:
        node.cleanup()
        node.destroy_node()
        rclpy.shutdown()


if __name__ == '__main__':
    main()
```

## 6.8 Learning Objectives

After this chapter, you will have:
- [ ] Built a complete perception pipeline with object detection
- [ ] Implemented a simple task planner
- [ ] Created a robot controller interface
- [ ] Integrated all components into a working system
- [ ] Optionally wrapped everything in ROS 2

## 6.9 Extension Ideas

1. **Add VLA Integration**: Replace the rule-based planner with an OpenVLA model
2. **Improve Perception**: Use segment anything (SAM) for better object detection
3. **Add Grasping**: Implement grasp pose estimation
4. **Multi-modal Commands**: Support voice input with Whisper
5. **Simulation Testing**: Integrate with Gazebo/Isaac Sim

## 6.10 Running the Project

```bash
# Setup
cd ai_robot_pipeline
pip install -r requirements.txt

# Run with webcam (simulation)
python src/main.py --sim --camera 0 --command "pick up the cup"

# Run with ROS 2
colcon build
ros2 run ai_robot_pipeline ros2_wrapper
ros2 topic pub /ai_robot/command std_msgs/String "data: 'pick up the cup'"
```

## Quiz

1. What are the main components of the AI-Robot Pipeline?
2. How does the TaskPlanner convert natural language to robot actions?
3. What interface does the RobotController provide?

---
*Previous: [Chapter 5 - VLA Systems](/docs/chapter-05-vla-systems) | [Back to Home](/docs/index)*
